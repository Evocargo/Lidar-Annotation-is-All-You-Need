from typing import Any, Optional, Sequence

import numpy as np
from prefetch_generator import BackgroundGenerator
from torch.utils.data import ConcatDataset, DataLoader, WeightedRandomSampler


class WeightedDataLoader(DataLoader):
    """Loader supporting dataset-wise weights to mix them in some proportion

    Overwritten torch.utils.data.DataLoader class with weighted over
    datasets batch generation functionality. Class works only with ConcatDataset
    class. The syntax is the same as default DataLoader.

    Note:
        Data will always be shuffled.

    Example:
        from torch.utils.data import ConcatDataset
        concat_dataset = ConcatDataset((dataset1, dataset2))
        loader = WeightedDataLoader(
            concat_dataset,
            (3, 8),
            batch_size=5,
            num_workers=4,
        )
    """

    def __init__(
        self,
        dataset: ConcatDataset,
        weights: Sequence[float],
        num_samples: Optional[int] = None,
        batch_size: int = 1,
        num_workers: int = 0,
        **kwargs: Any
    ):
        """
        Args:
            dataset: ConcatDataset to sample from.
            weights: weight for each sub-dataset from `dataset.datasets`.
            num_samples: overall number of samples generated by this loader
                (sum of all batches).

            The rest matches original `DataLoader` class.
        """
        self.dataset = dataset
        self.weights = weights
        self.num_samples = num_samples or len(self.dataset)
        weighted_sampler = WeightedRandomSampler(self.sample_weights, self.num_samples)
        super().__init__(
            dataset,
            batch_size,
            sampler=weighted_sampler,
            num_workers=num_workers,
            **kwargs,
        )

    @property
    def sample_weights(self) -> np.ndarray:
        """
        Create weights for sampling from the dataset.

        Returns:
            sample_weights (np.ndarray): An array of sample weights.
        """
        sample_weights = np.empty(len(self.dataset))
        start_ind = 0

        for weight, end_ind, dataset in zip(self.weights, self.dataset.cumulative_sizes, self.dataset.datasets):
            sample_weights[start_ind:end_ind] = weight / len(dataset)
            start_ind = end_ind

        return sample_weights


class DataLoaderX(DataLoader):
    """
    DataLoaderX extends the standard PyTorch DataLoader with background data prefetching
    using `BackgroundGenerator` for improved performance.
    """

    def __iter__(self) -> BackgroundGenerator:
        """
        Iterate over the DataLoader using the BackgroundGenerator to prefetch data.

        Returns:
            An iterator that prefetches data in the background.
        """
        return BackgroundGenerator(super().__iter__())
